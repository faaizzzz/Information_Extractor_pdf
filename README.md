Information Extraction from PDFs using QA System

 Problem Statement
Build a system that can process provided PDF documents and allow a user to ask questions about their content.
The system automatically extracts information and returns relevant answers.

Requirements
- Accept multiple PDF documents as input.
- Make the documents searchable.
- When a user asks a question in natural language, return the most relevant document sections.
- Provide a simple interface (CLI or notebook).



 Setup Instructions and how to run.

### 1Ô∏è‚É£ Install dependencies
```bash
pip install -r requirements.txt
## create a folder named 'pdfs' add the pdfs in this folder
##create a index for pdfs by running this code in your bash
python app.py --index pdfs --save-index index.pkl
##run QA mode by running this code in ypur bash
python app.py --load-index index.pkl

#will suggest to follow this steps
## follow the below steps to run in google colab
##install dependencies
!pip install pdfplumber scikit-learn numpy joblib
##upload app.py and indexer.py file after running the below code
from google.colab import files
uploaded = files.upload()  # upload app.py, indexer.py , and the pdfs in a folder called pdfs if not uploaded earlier.
##upload the pdfs in the Google Colab
##run the below code step by step to ask the questions
import sys
sys.path.append('/content')

main(index_folder="pdfs")    # This will build index.pkl from all PDFs


!python3 app.py
##the last step will allow you to ask questions

##tech stack

| Component         | Technology Used                  |
| ----------------- | -------------------------------- |
| Language          | Python 3                         |
| PDF Parsing       | `pdfplumber`                     |
| Vectorization     | `TfidfVectorizer` (scikit-learn) |
| Similarity Search | Cosine Similarity                |
| Model Persistence | `joblib`                         |
| CLI Interface     | Python argparse                  |
| (Optional) Web UI | Flask                            |


##example query
‚úÖ Index loaded successfully.

üí¨ QA Mode started. Press ENTER on an empty line to exit.

‚ùì Question: What are the five core functions of the Cybersecurity Framework?

üìò Auto-detected document: NIST.CSWP.04162018.pdf

[1] üîé From NIST.CSWP.04162018.pdf (score=0.5080):
‚Ä¢ as helpful in managing cybersecurity risk.
‚Ä¢ The Core comprises four elements: Functions, Categories, Subcategories, and Informative References, depicted in Figure 1: Figure 1: Framework Core Structure The Framework Core elements work together as follows: ÔÇ∑ Functions organize basic cybersecurity activities at their highest level.
‚Ä¢ These Functions are Identify, Protect, Detect, Respond, and Recover.
‚Ä¢ They aid an organization in expressing its management of cybersecurity risk by organizing information, enabling risk management decisions, addressing threats, and improving by learning from previous activities.
‚Ä¢ The Functions also align with existing methodologies for incident management and help show the impact of investments in cybersecurity.

[2] üîé From NIST.CSWP.04162018.pdf (score=0.4350):
‚Ä¢ 800-398, and the Electricity Subsector Cybersecurity Risk Management Process (RMP) guideline9.
‚Ä¢ 1.3 Document Overview The remainder of this document contains the following sections and appendices: ÔÇ∑ Section 2 describes the Framework components: the Framework Core, the Tiers, and the Profiles.
‚Ä¢ ÔÇ∑ Section 3 presents examples of how the Framework can be used.
‚Ä¢ ÔÇ∑ Section 4 describes how to use the Framework for self-assessing and demonstrating cybersecurity through measurements.
‚Ä¢ ÔÇ∑ Appendix A presents the Framework Core in a tabular format: the Functions, Categories, Subcategories, and Informative References.
‚ùì Question: What are the main components of the Cybersecurity Framework?

üìò Auto-detected document: NIST.CSWP.04162018.pdf

[1] üîé From NIST.CSWP.04162018.pdf (score=0.2759):
‚Ä¢ Ultimately, the Framework is aimed at reducing and better managing cybersecurity risks.
‚Ä¢ April 16, 2018 Cybersecurity Framework Version 1.1 This publication is available free of charge from: https://doi.org/10.6028/NIST.CSWP.04162018 3 To account for the unique cybersecurity needs of organizations, there are a wide variety of ways to use the Framework.
‚Ä¢ The decision about how to apply it is left to the implementing organization.
‚Ä¢ For example, one organization may choose to use the Framework Implementation Tiers to articulate envisioned risk management practices.
‚Ä¢ Another organization may use the Framework‚Äôs five Functions to analyze its entire risk management portfolio; that analysis may or may not rely on more detailed companion guidance, such as controls catalogs.

[2] üîé From NIST.CSWP.04162018.pdf (score=0.2725):
‚Ä¢ people or organizations that consume a given product or service.
‚Ä¢ Category The subdivision of a Function into groups of cybersecurity outcomes, closely tied to programmatic needs and particular activities.
‚Ä¢ Examples of Categories include ‚ÄúAsset Management,‚Äù ‚ÄúIdentity Management and Access Control,‚Äù and ‚ÄúDetection Processes.‚Äù Critical Infrastructure Systems and assets, whether physical or virtual, so vital to the United States that the incapacity or destruction of such systems and assets would have a debilitating impact on cybersecurity, national economic security, national public health or safety, or any combination of those matters.
‚Ä¢ Cybersecurity The process of protecting information by preventing, detecting, and responding to attacks.
‚Ä¢ Cybersecurity Event A cybersecurity change that may have an impact on organizational operations (including mission, capabilities, or reputation).

‚ùì Question: What is multi-head attention and why is it useful?

üìò Auto-detected document: 1706.03762v7.pdf

[1] üîé From 1706.03762v7.pdf (score=0.2446):
‚Ä¢ of Figure 1, respectively.
‚Ä¢ 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers.
‚Ä¢ Each layer has two sub-layers.
‚Ä¢ The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network.
‚Ä¢ We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1].

[2] üîé From 1706.03762v7.pdf (score=0.2130):
‚Ä¢ softmax function to obtain the weights on the values.
‚Ä¢ In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.
‚Ä¢ The keys and values are also packed together into matrices K and V .
‚Ä¢ We compute the matrix of outputs as: Attention(Q, K, V ) = softmax(QKT ‚àödk )V (1) The two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention.
‚Ä¢ Dot-product attention is identical to our algorithm, except for the scaling factor of 1 ‚àödk .

‚ùì Question: What are the two training objectives of BERT?

üìò Auto-detected document: 1810.04805v2.pdf

[1] üîé From 1810.04805v2.pdf (score=0.2118):
‚Ä¢ The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.
‚Ä¢ Training of BERTBASE was performed on 4 Cloud TPUs in Pod conÔ¨Åguration (16 TPU chips total).13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips total).
‚Ä¢ Each pre- training took 4 days to complete.
‚Ä¢ Longer sequences are disproportionately expen- sive because attention is quadratic to the sequence length.
‚Ä¢ To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps.

[2] üîé From 1810.04805v2.pdf (score=0.1828):
‚Ä¢ possible so that the two methods could be minimally compared.
‚Ä¢ The core argument of this work is that the bi-directionality and the two pre- training tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained: ‚Ä¢ GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCor- pus (800M words) and Wikipedia (2,500M words).
‚Ä¢ ‚Ä¢ GPT uses a sentence separator ([SEP]) and classiÔ¨Åer token ([CLS]) which are only in- troduced at Ô¨Åne-tuning time; BERT learns [SEP], [CLS] and sentence A/B embed- dings during pre-training.
‚Ä¢ ‚Ä¢ GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words.
‚Ä¢ ‚Ä¢ GPT used the same learning rate of 5e-5 for all Ô¨Åne-tuning experiments; BERT chooses a task-speciÔ¨Åc Ô¨Åne-tuning learning rate which performs the best on the development set.




